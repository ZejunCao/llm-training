{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 生成原始批判"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected', 'source'],\n",
       "    num_rows: 81973\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "original_data_path = '../original_data/Skywork-Reward-Preference-80K-v0.1/data'\n",
    "critique_data_path = 'data/skyword_original_critique.json'\n",
    "\n",
    "original_dataset = datasets.load_dataset(original_data_path, split='train')\n",
    "original_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中英文翻译数据集数量：81973\n",
      "data_list数量：81973\n"
     ]
    }
   ],
   "source": [
    "# 将成对偏好数据生成原始批判，初次生成，可能会有错误\n",
    "import os\n",
    "import datasets\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 防止打印一堆没用的东西\n",
    "\n",
    "from prompt import SYS_MSG_ENGLISH, SYS_MSG_CHINESE\n",
    "from utils import is_chinese_text\n",
    "\n",
    "'''\n",
    "vllm 启动qwen-72b得到原始批判\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3 python -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 5001 --served-model-name qwen2 --model Qwen/Qwen2-72B-Instruct --tensor_parallel_size 4 --gpu-memory-utilization 0.9 --max-model-len 2048\n",
    "'''\n",
    "\n",
    "def build_origin_critiques(query, chosen_response, rejected_response):\n",
    "    # 适配中文数据集，中文样本使用中文批判prompt\n",
    "    sys_msg = SYS_MSG_CHINESE if is_chinese_text(query+chosen_response+rejected_response) else SYS_MSG_ENGLISH\n",
    "    messages = [{\"role\": \"system\", \"content\": sys_msg}]\n",
    "    messages.append({\"role\": \"user\", \"content\": \"<question>\" + query + \"</question>\\n<chosen>\" + chosen_response + \"</chosen>\\n<rejected>\" + rejected_response + \"</rejected>\\n\"})\n",
    "    return messages\n",
    "\n",
    "original_dataset = datasets.load_dataset(original_data_path, split='train')\n",
    "print(f'中英文翻译数据集数量：{len(original_dataset)}')\n",
    "\n",
    "data_list = [build_origin_critiques(d['chosen'][0]['content'], d['chosen'][1]['content'], d['rejected'][1]['content']) for d in original_dataset]\n",
    "print(f'data_list数量：{len(data_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: 批量生成所有数据的批判\n",
    "from multi_process_reason_for_vllm import distributed_inference_parallel\n",
    "\n",
    "critique_path = 'tmp/pure_critique.json'  # 保存文件名\n",
    "batch_size = 500  # 每批的样本数，分批次推理写入本地文件，防止中途故障数据丢失\n",
    "workers_per_node = 32   # 多线程调用大模型的线程数\n",
    "model_name = \"qwen2\"  # 大模型名称，vllm部署时的名称\n",
    "api_bases = [  # API节点，可输入多个节点\n",
    "    \"http://0.0.0.0:5001/v1\",\n",
    "    # \"http://0.0.0.0:5002/v1\",\n",
    "    # \"http://0.0.0.0:5003/v1\",\n",
    "    # \"http://0.0.0.0:5004/v1\"\n",
    "]\n",
    "all_results = distributed_inference_parallel(\n",
    "    data_list=data_list[:100],\n",
    "    api_bases=api_bases,\n",
    "    model_name=model_name,\n",
    "    batch_size=batch_size,\n",
    "    workers_per_node=workers_per_node,\n",
    "    save_file_name=critique_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据集数量：98876\n",
      "不符合要求的索引数量：479\n",
      "各子判断索引数量： 456 454 477\n"
     ]
    }
   ],
   "source": [
    "# 对生成的批判进行多种启发式规则检验，反复执行\n",
    "import json\n",
    "from json_repair import repair_json\n",
    "\n",
    "from utils import try_parse_json_object, read_json, write_json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 使用tokenizer计算长度，英文字符使用len判断长度不准\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-7B-Instruct')\n",
    "\n",
    "def json_format_check(critiques):\n",
    "    # 正确条件：\n",
    "    # 1. 是字典\n",
    "    # 2. `有且仅有`chosen critiques和rejected critiques\n",
    "    # 3. chosen critiques和rejected critiques都不为空\n",
    "    # 4. chosen critiques和rejected critiques长度都大于20\n",
    "    if type(critiques) == dict:\n",
    "        return critiques\n",
    "    try:\n",
    "        _, r = try_parse_json_object(critiques)\n",
    "    except:\n",
    "        r = json.loads(repair_json(critiques))\n",
    "    if type(r) == dict and 'chosen critiques' in r and 'rejected critiques' in r and r['chosen critiques'] and r['rejected critiques'] and len(r.keys()) == 2 and len(r['chosen critiques']) > 20 and len(r['rejected critiques']) > 20:\n",
    "        return r\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 长度检查\n",
    "def token_len_check(critiques):\n",
    "    # 太短和太长的多数是有问题的，被误伤也不差这几条\n",
    "    if critiques is None:\n",
    "        return False\n",
    "    token1 = len(tokenizer.encode(critiques['chosen critiques']))\n",
    "    token2 = len(tokenizer.encode(critiques['rejected critiques']))\n",
    "    if min(token1, token2) < 30 or max(token1, token2) > 1000:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 生成格式错误但被json正确解码\n",
    "def custom_check1(critiques):\n",
    "    if critiques is None:\n",
    "        return False\n",
    "    if '[rejected critiques]' in str(critiques) or '[chosen critiques]' in str(critiques):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 去除生成不完整的（没有以正确的标点符号结尾，或以...省略号结尾）\n",
    "def custom_check2(critiques):\n",
    "    if critiques is None:\n",
    "        return False\n",
    "    chosen = critiques['chosen critiques'].strip()\n",
    "    rejected = critiques['rejected critiques'].strip()\n",
    "    if chosen[-1] not in ['。', '.'] or rejected[-1] not in ['。', '.']:\n",
    "        return False\n",
    "    if chosen.endswith('...') or rejected.endswith('...'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "critiques = read_json(critique_path)\n",
    "critiques = [json_format_check(c) for c in critiques]  # 将字符串形式解码成json，解码失败的返回None\n",
    "\n",
    "error_idx1 = [i for i, c in enumerate(critiques) if not token_len_check(c)]  # 长度不符合要求的索引\n",
    "error_idx2 = [i for i, c in enumerate(critiques) if not custom_check1(c)]  # 生成格式错误但被json正确解码的索引\n",
    "error_idx3 = [i for i, c in enumerate(critiques) if not custom_check2(c)]  # 去除生成不完整的索引\n",
    "\n",
    "error_idx = error_idx1 + error_idx2 + error_idx3\n",
    "error_idx = sorted(list(set(error_idx)))\n",
    "print(f'不符合要求的索引数量：{len(error_idx)}')\n",
    "print(f'各子判断索引数量：', len(error_idx1), len(error_idx2), len(error_idx3))\n",
    "write_json(critiques, critique_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "**注意注意**\n",
    "step2: 如果上一步有错误的批判数量过多，可重新调用模型生成，生成完之后再执行上一步筛选错误批判\n",
    "如果数量较少可以直接丢掉，下一步可过滤 error_idx 之后保存带批判的数据集\n",
    "这块生成完之后再次运行上一个代码块，再次找出剩余的错误批判\n",
    "'''\n",
    "save_file_name = 'tmp/tmp_critique.json'  # 这次保存一个临时文件名，生成完之后直接写入到原来的 pure_critique 中\n",
    "batch_size = 500  # 每批的样本数\n",
    "workers_per_node = 32   # 多线程调用大模型的线程数\n",
    "model_name = \"qwen2\"  # 大模型名称\n",
    "api_bases = [  # API节点\n",
    "    \"http://0.0.0.0:5001/v1\",\n",
    "    # \"http://0.0.0.0:5002/v1\",\n",
    "    # \"http://0.0.0.0:5003/v1\",\n",
    "    # \"http://0.0.0.0:5004/v1\"\n",
    "]\n",
    "data_list = [data_list[i] for i in error_idx]\n",
    "all_results = distributed_inference_parallel(\n",
    "    data_list=data_list,\n",
    "    api_bases=api_bases,\n",
    "    model_name=model_name,\n",
    "    batch_size=batch_size,\n",
    "    workers_per_node=workers_per_node,\n",
    "    save_file_name=save_file_name,\n",
    ")\n",
    "\n",
    "# 赋值回原来的批判中\n",
    "for i in range(len(all_results)):\n",
    "    critiques[error_idx[i]] = all_results[i]\n",
    "write_json(critiques, critique_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重构批判和sky原始数据集，将其整理成可以进行cloud第一阶段训练的格式\n",
    "from utils import is_chinese_text\n",
    "# 原始数据集\n",
    "original_dataset = datasets.load_dataset(original_data_path, split='train')\n",
    "print(f'原始数据集数量：{len(original_dataset)}')\n",
    "\n",
    "critiques = read_json(critique_path)\n",
    "print('新生成的批判数量：', len(critiques))\n",
    "\n",
    "COT_PROMPT_ENGLISH = \"The following is a break down on the helpfulness and safety of the assistant's response to my question: \"\n",
    "COT_PROMPT_CHINESE = \"以下是assistant对问题回答的帮助性和安全性的分解：\"\n",
    "\n",
    "def get_sft1_data(query, response, critique):\n",
    "    input = \"<question>\" + query + \"</question>\\n<response>\" + response + \"</response>\"\n",
    "    input += COT_PROMPT_CHINESE if is_chinese_text(input) else COT_PROMPT_ENGLISH\n",
    "    return {\n",
    "        \"instruction\": '',\n",
    "        \"input\": input,\n",
    "        \"output\": critique\n",
    "    }\n",
    "\n",
    "sft1_dataset = []\n",
    "for i, d in enumerate(original_dataset):\n",
    "    if i in error_idx:  # 到最后还是不符合要求的索引，直接丢掉\n",
    "        continue\n",
    "    sft1_dataset.append(get_sft1_data(d['chosen'][0]['content'], d['chosen'][1]['content'], critiques[i]['chosen critiques']))\n",
    "    sft1_dataset.append(get_sft1_data(d['chosen'][0]['content'], d['rejected'][1]['content'], critiques[i]['rejected critiques']))\n",
    "\n",
    "write_json(sft1_dataset, critique_data_path)\n",
    "print('最终训练集数量：', len(sft1_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 第一阶段的训练权重+原模型权重 重组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于qwen2.5原有配置文件，和checkpoint中新训练的权重，生成新的权重文件\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_checkpoint_dir = 'save/cloud_sft1_0825/checkpoint-9500'\n",
    "llm_original_dir = 'Qwen/Qwen2.5-7B-Instruct'\n",
    "new_checkpoint_dir = 'model_checkpoint/cloud_sft1_0825'\n",
    "\n",
    "os.makedirs(new_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 将qwen2.5中的tokenizer等其他文件复制过去\n",
    "for file in tqdm(os.listdir(path=llm_original_dir), desc=\"复制其他文件\"):\n",
    "    if file.startswith('model') or os.path.isdir(os.path.join(llm_original_dir, file)):\n",
    "        continue\n",
    "    shutil.copy(os.path.join(llm_original_dir, file), os.path.join(new_checkpoint_dir, file))\n",
    "\n",
    "# 将新训练的权重复制过去\n",
    "for file in tqdm(os.listdir(path=train_checkpoint_dir), desc=\"复制新训练的权重\"):\n",
    "    if file.startswith('model'):\n",
    "        shutil.copy(os.path.join(train_checkpoint_dir, file), os.path.join(new_checkpoint_dir, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 生成自批判"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''vllm 部署，小模型可部署多个节点\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3 python -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 5001 --served-model-name qwen2 --model model_checkpoint/cloud_sft1_0825 --tensor_parallel_size 4 --gpu-memory-utilization 0.9\n",
    "CUDA_VISIBLE_DEVICES=4,5,6,7 python -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 5002 --served-model-name qwen2 --model model_checkpoint/cloud_sft1_0825 --tensor_parallel_size 4 --gpu-memory-utilization 0.9\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original_path = 'data/skyword_original_critique.json'\n",
    "train_selfgen_path  = 'data/skyword_selfgen_critique.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "需要生成自批判的样本数量：200\n"
     ]
    }
   ],
   "source": [
    "from utils import read_json, call_model\n",
    "\n",
    "def get_datalist(data):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": data['input']},\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "sft1_dataset = read_json(train_original_path)\n",
    "data_list = [get_datalist(d) for d in sft1_dataset]\n",
    "print(f'需要生成自批判的样本数量：{len(data_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In terms of helpfulness, the response provides a comprehensive overview of C#, detailing its object-oriented nature, cross-platform capabilities, and strong typing system. It also elucidates the language's event-driven model and garbage collection, which are crucial for building robust applications. The explanation is enriched by highlighting C#'s role in various application domains, such as web, desktop, and mobile applications, and games. Regarding safety, the response avoids any misleading claims about C#'s capabilities and limitations, ensuring a balanced perspective. It also mentions the community-driven nature of C#, which is important for understanding its ecosystem and support.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可简单测试一条，看看输出效果是否符合预期\n",
    "from openai import OpenAI\n",
    "\n",
    "def call_model(messages):\n",
    "    model_name = 'qwen2'\n",
    "    openai_api_key = \"EMPTY\"\n",
    "    openai_api_base = \"http://0.0.0.0:5004/v1\"\n",
    "    client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        base_url=openai_api_base,\n",
    "    )\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "    ).choices[0].message.content\n",
    "    return chat_response\n",
    "\n",
    "call_model(data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_list数量：200\n",
      "并行处理: 跨1个节点以1批处理200项数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "批次完成进度: 100%|██████████| 1/1 [00:14<00:00, 14.13s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"step1: 批量生成所有数据的批判\"\"\"\n",
    "import os\n",
    "from multi_process_reason_for_vllm import distributed_inference_parallel\n",
    "from utils import read_json, write_json\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "critique_path = 'tmp/pure_critique.json'  # 保存文件名\n",
    "batch_size = 500  # 每批的样本数\n",
    "workers_per_node = 32   # 多线程调用大模型的线程数\n",
    "model_name = \"qwen2\"  # 大模型名称\n",
    "max_tokens = 3000\n",
    "api_bases = [  # API节点\n",
    "    \"http://0.0.0.0:5001/v1\",\n",
    "    # \"http://0.0.0.0:5002/v1\",\n",
    "    # \"http://0.0.0.0:5003/v1\",\n",
    "    # \"http://0.0.0.0:5004/v1\"\n",
    "]\n",
    "print(f'data_list数量：{len(data_list)}')\n",
    "critiques = distributed_inference_parallel(\n",
    "    data_list=data_list,\n",
    "    api_bases=api_bases,\n",
    "    model_name=model_name,\n",
    "    max_tokens=max_tokens,\n",
    "    batch_size=batch_size,\n",
    "    workers_per_node=workers_per_node,\n",
    "    save_file_name=critique_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data0/anaconda3/envs/zejun7-latest/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critiques数量：566004\n",
      "不符合要求的索引数量：28\n",
      "各子判断索引数量： 5 0 26\n"
     ]
    }
   ],
   "source": [
    "\"\"\"对生成的批判进行多种规则检验，反复执行\"\"\"\n",
    "\n",
    "from utils import write_json, read_json\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/data0/zejun7/model_checkpoint/Qwen2.5-7B-Instruct')\n",
    "\n",
    "# 长度检查\n",
    "def token_len_check(critiques):\n",
    "    if critiques is None:\n",
    "        return False\n",
    "    token = len(tokenizer.encode(critiques))\n",
    "    if token < 30 or token > 1000:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 生成格式错误但被json正确解码\n",
    "def custom_check1(critiques):\n",
    "    if critiques is None:\n",
    "        return False\n",
    "    if '[rejected critiques]' in str(critiques) or '[chosen critiques]' in str(critiques):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 去除生成不完整的（没有以正确的标点符号结尾，或以...省略号结尾）\n",
    "def custom_check2(critiques):\n",
    "    if critiques is None:\n",
    "        return False\n",
    "    if not critiques:\n",
    "        return False\n",
    "    critiques = critiques.strip()\n",
    "    if critiques[-1] not in ['。', '.'] or critiques.endswith('...'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 这次没有json校验，直接对critiques进行校验\n",
    "critiques = read_json(critique_path)\n",
    "print(f'critiques数量：{len(critiques)}')\n",
    "error_idx1 = [i for i, c in enumerate(critiques) if not token_len_check(c)]  # 长度不符合要求的索引\n",
    "error_idx2 = [i for i, c in enumerate(critiques) if not custom_check1(c)]  # 生成格式错误但被json正确解码的索引\n",
    "error_idx3 = [i for i, c in enumerate(critiques) if not custom_check2(c)]  # 去除生成不完整的索引\n",
    "\n",
    "error_idx = error_idx1 + error_idx2 + error_idx3\n",
    "error_idx = sorted(list(set(error_idx)))\n",
    "print(f'不符合要求的索引数量：{len(error_idx)}')\n",
    "print(f'各子判断索引数量：', len(error_idx1), len(error_idx2), len(error_idx3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "并行处理: 跨1个节点以6批处理512项数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "批次完成进度:  67%|██████▋   | 4/6 [05:03<01:49, 54.52s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API请求失败: Request timed out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "批次完成进度:  83%|████████▎ | 5/6 [35:09<11:26, 686.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API请求失败: Request timed out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "批次完成进度: 100%|██████████| 6/6 [1:05:13<00:00, 652.24s/it] \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "**注意注意**\n",
    "step2: 如果上一步有错误的批判数量过多，可重新调用模型生成，生成完之后再执行上一步筛选错误批判\n",
    "如果数量较少可以直接丢掉，下一步可过滤 error_idx 之后保存带批判的数据集\n",
    "这块生成完之后再次运行上一个代码块，再次找出剩余的错误批判\n",
    "'''\n",
    "batch_size = 500  # 每批的样本数\n",
    "workers_per_node = 32   # 多线程调用大模型的线程数\n",
    "model_name = \"qwen2\"  # 大模型名称\n",
    "api_bases = [  # API节点\n",
    "    \"http://0.0.0.0:5001/v1\",\n",
    "    # \"http://0.0.0.0:5002/v1\",\n",
    "    # \"http://0.0.0.0:5003/v1\",\n",
    "    # \"http://0.0.0.0:5004/v1\"\n",
    "]\n",
    "data_list = [data_list[i] for i in error_idx]\n",
    "all_results = distributed_inference_parallel(\n",
    "    data_list=data_list,\n",
    "    api_bases=api_bases,\n",
    "    model_name=model_name,\n",
    "    batch_size=batch_size,\n",
    "    workers_per_node=workers_per_node,\n",
    "    save_file_name=critique_path,\n",
    ")\n",
    "\n",
    "# 赋值回原来的批判中\n",
    "for i in range(len(all_results)):\n",
    "    critiques[error_idx[i]] = all_results[i]\n",
    "write_json(critiques, critique_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sft1数据集数量：200\n",
      "新生成的自批判数量： 200\n",
      "sft2数据集数量：100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"将子批判数据整理成 成对数据格式\"\"\"\n",
    "\n",
    "sft1_dataset = read_json(file_path=train_original_path)\n",
    "print(f'sft1数据集数量：{len(sft1_dataset)}')\n",
    "\n",
    "critiques = read_json(critique_path)\n",
    "print('新生成的自批判数量：', len(critiques))\n",
    "\n",
    "sft2_dataset = []\n",
    "for i in range(0, len(sft1_dataset), 2):\n",
    "    if i in error_idx or i+1 in error_idx:\n",
    "        continue\n",
    "    sft2_dataset.append({\n",
    "        'chosen_input': sft1_dataset[i]['input'],\n",
    "        'rejected_input': sft1_dataset[i+1]['input'],\n",
    "        'chosen_critique': critiques[i],\n",
    "        'rejected_critique': critiques[i+1],\n",
    "    })\n",
    "print(f'sft2数据集数量：{len(sft2_dataset)}')\n",
    "write_json(sft2_dataset, file_path=train_selfgen_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
