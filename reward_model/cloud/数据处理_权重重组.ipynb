{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 生成原始批判"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected', 'source'],\n",
       "    num_rows: 81973\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "original_data_path = '../original_data/Skywork-Reward-Preference-80K-v0.1/data'\n",
    "critique_data_path = 'data/skyword_original_critique.json'\n",
    "\n",
    "original_dataset = datasets.load_dataset(original_data_path, split='train')\n",
    "original_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中英文翻译数据集数量：81973\n",
      "data_list数量：81973\n"
     ]
    }
   ],
   "source": [
    "# 将成对偏好数据生成原始批判，初次生成，可能会有错误\n",
    "import os\n",
    "import datasets\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 防止打印一堆没用的东西\n",
    "\n",
    "from prompt import SYS_MSG_ENGLISH, SYS_MSG_CHINESE\n",
    "from utils import is_chinese_text\n",
    "\n",
    "'''\n",
    "vllm 启动qwen-72b得到原始批判\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3 python -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 5001 --served-model-name qwen2 --model Qwen/Qwen2-72B-Instruct --tensor_parallel_size 4 --gpu-memory-utilization 0.9 --max-model-len 2048\n",
    "'''\n",
    "\n",
    "def build_origin_critiques(query, chosen_response, rejected_response):\n",
    "    # 适配中文数据集，中文样本使用中文批判prompt\n",
    "    sys_msg = SYS_MSG_CHINESE if is_chinese_text(query+chosen_response+rejected_response) else SYS_MSG_ENGLISH\n",
    "    messages = [{\"role\": \"system\", \"content\": sys_msg}]\n",
    "    messages.append({\"role\": \"user\", \"content\": \"<question>\" + query + \"</question>\\n<chosen>\" + chosen_response + \"</chosen>\\n<rejected>\" + rejected_response + \"</rejected>\\n\"})\n",
    "    return messages\n",
    "\n",
    "original_dataset = datasets.load_dataset(original_data_path, split='train')\n",
    "print(f'中英文翻译数据集数量：{len(original_dataset)}')\n",
    "\n",
    "data_list = [build_origin_critiques(d['chosen'][0]['content'], d['chosen'][1]['content'], d['rejected'][1]['content']) for d in original_dataset]\n",
    "print(f'data_list数量：{len(data_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: 批量生成所有数据的批判\n",
    "from multi_process_reason_for_vllm import distributed_inference_parallel\n",
    "\n",
    "critique_path = 'tmp/pure_critique.json'  # 保存文件名\n",
    "batch_size = 500  # 每批的样本数，分批次推理写入本地文件，防止中途故障数据丢失\n",
    "workers_per_node = 32   # 多线程调用大模型的线程数\n",
    "model_name = \"qwen2\"  # 大模型名称，vllm部署时的名称\n",
    "api_bases = [  # API节点，可输入多个节点\n",
    "    \"http://0.0.0.0:5001/v1\",\n",
    "    # \"http://0.0.0.0:5002/v1\",\n",
    "    # \"http://0.0.0.0:5003/v1\",\n",
    "    # \"http://0.0.0.0:5004/v1\"\n",
    "]\n",
    "all_results = distributed_inference_parallel(\n",
    "    data_list=data_list[:100],\n",
    "    api_bases=api_bases,\n",
    "    model_name=model_name,\n",
    "    batch_size=batch_size,\n",
    "    workers_per_node=workers_per_node,\n",
    "    save_file_name=critique_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据集数量：98876\n",
      "不符合要求的索引数量：479\n",
      "各子判断索引数量： 456 454 477\n"
     ]
    }
   ],
   "source": [
    "# 对生成的批判进行多种启发式规则检验，反复执行\n",
    "import json\n",
    "from json_repair import repair_json\n",
    "\n",
    "from utils import try_parse_json_object, read_json, write_json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 使用tokenizer计算长度，英文字符使用len判断长度不准\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-7B-Instruct')\n",
    "\n",
    "def json_format_check(critiques):\n",
    "    # 正确条件：\n",
    "    # 1. 是字典\n",
    "    # 2. `有且仅有`chosen critiques和rejected critiques\n",
    "    # 3. chosen critiques和rejected critiques都不为空\n",
    "    # 4. chosen critiques和rejected critiques长度都大于20\n",
    "    if type(critiques) == dict:\n",
    "        return critiques\n",
    "    try:\n",
    "        _, r = try_parse_json_object(critiques)\n",
    "    except:\n",
    "        r = json.loads(repair_json(critiques))\n",
    "    if type(r) == dict and 'chosen critiques' in r and 'rejected critiques' in r and r['chosen critiques'] and r['rejected critiques'] and len(r.keys()) == 2 and len(r['chosen critiques']) > 20 and len(r['rejected critiques']) > 20:\n",
    "        return r\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 长度检查\n",
    "def token_len_check(critiques):\n",
    "    # 太短和太长的多数是有问题的，被误伤也不差这几条\n",
    "    if critiques is None:\n",
    "        return False\n",
    "    token1 = len(tokenizer.encode(critiques['chosen critiques']))\n",
    "    token2 = len(tokenizer.encode(critiques['rejected critiques']))\n",
    "    if min(token1, token2) < 30 or max(token1, token2) > 1000:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 生成格式错误但被json正确解码\n",
    "def custom_check1(critiques):\n",
    "    if critiques is None:\n",
    "        return False\n",
    "    if '[rejected critiques]' in str(critiques) or '[chosen critiques]' in str(critiques):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 去除生成不完整的（没有以正确的标点符号结尾，或以...省略号结尾）\n",
    "def custom_check2(critiques):\n",
    "    if critiques is None:\n",
    "        return False\n",
    "    chosen = critiques['chosen critiques'].strip()\n",
    "    rejected = critiques['rejected critiques'].strip()\n",
    "    if chosen[-1] not in ['。', '.'] or rejected[-1] not in ['。', '.']:\n",
    "        return False\n",
    "    if chosen.endswith('...') or rejected.endswith('...'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "critiques = read_json(critique_path)\n",
    "critiques = [json_format_check(c) for c in critiques]  # 将字符串形式解码成json，解码失败的返回None\n",
    "\n",
    "error_idx1 = [i for i, c in enumerate(critiques) if not token_len_check(c)]  # 长度不符合要求的索引\n",
    "error_idx2 = [i for i, c in enumerate(critiques) if not custom_check1(c)]  # 生成格式错误但被json正确解码的索引\n",
    "error_idx3 = [i for i, c in enumerate(critiques) if not custom_check2(c)]  # 去除生成不完整的索引\n",
    "\n",
    "error_idx = error_idx1 + error_idx2 + error_idx3\n",
    "error_idx = sorted(list(set(error_idx)))\n",
    "print(f'不符合要求的索引数量：{len(error_idx)}')\n",
    "print(f'各子判断索引数量：', len(error_idx1), len(error_idx2), len(error_idx3))\n",
    "write_json(critiques, critique_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "**注意注意**\n",
    "step2: 如果上一步有错误的批判数量过多，可重新调用模型生成，生成完之后再执行上一步筛选错误批判\n",
    "如果数量较少可以直接丢掉，下一步可过滤 error_idx 之后保存带批判的数据集\n",
    "这块生成完之后再次运行上一个代码块，再次找出剩余的错误批判\n",
    "'''\n",
    "save_file_name = 'tmp/tmp_critique.json'  # 这次保存一个临时文件名，生成完之后直接写入到原来的 pure_critique 中\n",
    "batch_size = 500  # 每批的样本数\n",
    "workers_per_node = 32   # 多线程调用大模型的线程数\n",
    "model_name = \"qwen2\"  # 大模型名称\n",
    "api_bases = [  # API节点\n",
    "    \"http://0.0.0.0:5001/v1\",\n",
    "    # \"http://0.0.0.0:5002/v1\",\n",
    "    # \"http://0.0.0.0:5003/v1\",\n",
    "    # \"http://0.0.0.0:5004/v1\"\n",
    "]\n",
    "data_list = [data_list[i] for i in error_idx]\n",
    "all_results = distributed_inference_parallel(\n",
    "    data_list=data_list,\n",
    "    api_bases=api_bases,\n",
    "    model_name=model_name,\n",
    "    batch_size=batch_size,\n",
    "    workers_per_node=workers_per_node,\n",
    "    save_file_name=save_file_name,\n",
    ")\n",
    "\n",
    "# 赋值回原来的批判中\n",
    "for i in range(len(all_results)):\n",
    "    critiques[error_idx[i]] = all_results[i]\n",
    "write_json(critiques, critique_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重构批判和sky原始数据集，将其整理成可以进行cloud第一阶段训练的格式\n",
    "from utils import is_chinese_text\n",
    "# 原始数据集\n",
    "original_dataset = datasets.load_dataset(original_data_path, split='train')\n",
    "print(f'原始数据集数量：{len(original_dataset)}')\n",
    "\n",
    "critiques = read_json(critique_path)\n",
    "print('新生成的批判数量：', len(critiques))\n",
    "\n",
    "COT_PROMPT_ENGLISH = \"The following is a break down on the helpfulness and safety of the assistant's response to my question: \"\n",
    "COT_PROMPT_CHINESE = \"以下是assistant对问题回答的帮助性和安全性的分解：\"\n",
    "\n",
    "def get_sft1_data(query, response, critique):\n",
    "    input = \"<question>\" + query + \"</question>\\n<response>\" + response + \"</response>\"\n",
    "    input += COT_PROMPT_CHINESE if is_chinese_text(input) else COT_PROMPT_ENGLISH\n",
    "    return {\n",
    "        \"instruction\": '',\n",
    "        \"input\": input,\n",
    "        \"output\": critique\n",
    "    }\n",
    "\n",
    "sft1_dataset = []\n",
    "for i, d in enumerate(original_dataset):\n",
    "    if i in error_idx:  # 到最后还是不符合要求的索引，直接丢掉\n",
    "        continue\n",
    "    sft1_dataset.append(get_sft1_data(d['chosen'][0]['content'], d['chosen'][1]['content'], critiques[i]['chosen critiques']))\n",
    "    sft1_dataset.append(get_sft1_data(d['chosen'][0]['content'], d['rejected'][1]['content'], critiques[i]['rejected critiques']))\n",
    "\n",
    "write_json(sft1_dataset, critique_data_path)\n",
    "print('最终训练集数量：', len(sft1_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 第一阶段训练权重+原模型权重 重组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于qwen2.5原有配置文件，和checkpoint中新训练的权重，生成新的权重文件\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_checkpoint_dir = 'save/cloud_sft1_0825/checkpoint-9500'\n",
    "llm_original_dir = 'Qwen/Qwen2.5-7B-Instruct'\n",
    "new_checkpoint_dir = 'model_checkpoint/cloud_sft1_0825'\n",
    "\n",
    "os.makedirs(new_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 将qwen2.5中的tokenizer等其他文件复制过去\n",
    "for file in tqdm(os.listdir(path=llm_original_dir), desc=\"复制其他文件\"):\n",
    "    if file.startswith('model') or os.path.isdir(os.path.join(llm_original_dir, file)):\n",
    "        continue\n",
    "    shutil.copy(os.path.join(llm_original_dir, file), os.path.join(new_checkpoint_dir, file))\n",
    "\n",
    "# 将新训练的权重复制过去\n",
    "for file in tqdm(os.listdir(path=train_checkpoint_dir), desc=\"复制新训练的权重\"):\n",
    "    if file.startswith('model'):\n",
    "        shutil.copy(os.path.join(train_checkpoint_dir, file), os.path.join(new_checkpoint_dir, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 生成自批判"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''vllm 部署，小模型可部署多个节点\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3 python -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 5001 --served-model-name qwen2 --model model_checkpoint/cloud_sft1_0825 --tensor_parallel_size 4 --gpu-memory-utilization 0.9\n",
    "CUDA_VISIBLE_DEVICES=4,5,6,7 python -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 5002 --served-model-name qwen2 --model model_checkpoint/cloud_sft1_0825 --tensor_parallel_size 4 --gpu-memory-utilization 0.9\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original_path = 'data/skyword_original_critique.json'\n",
    "train_selfgen_path  = 'data/skyword_selfgen_critique.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "需要生成自批判的样本数量：200\n"
     ]
    }
   ],
   "source": [
    "from utils import read_json, call_model\n",
    "\n",
    "def get_datalist(data):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": data['input']},\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "sft1_dataset = read_json(train_original_path)\n",
    "data_list = [get_datalist(d) for d in sft1_dataset]\n",
    "print(f'需要生成自批判的样本数量：{len(data_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In terms of helpfulness, the response provides a comprehensive overview of C#, detailing its object-oriented nature, cross-platform capabilities, and strong typing system. It also elucidates the language's event-driven model and garbage collection, which are crucial for building robust applications. The explanation is enriched by highlighting C#'s role in various application domains, such as web, desktop, and mobile applications, and games. Regarding safety, the response avoids any misleading claims about C#'s capabilities and limitations, ensuring a balanced perspective. It also mentions the community-driven nature of C#, which is important for understanding its ecosystem and support.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可简单测试一条，看看输出效果是否符合预期\n",
    "from openai import OpenAI\n",
    "\n",
    "def call_model(messages):\n",
    "    model_name = 'qwen2'\n",
    "    openai_api_key = \"EMPTY\"\n",
    "    openai_api_base = \"http://0.0.0.0:5004/v1\"\n",
    "    client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        base_url=openai_api_base,\n",
    "    )\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "    ).choices[0].message.content\n",
    "    return chat_response\n",
    "\n",
    "call_model(data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_list数量：200\n",
      "并行处理: 跨1个节点以1批处理200项数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "批次完成进度: 100%|██████████| 1/1 [00:14<00:00, 14.13s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"step1: 批量生成所有数据的批判\"\"\"\n",
    "import os\n",
    "from multi_process_reason_for_vllm import distributed_inference_parallel\n",
    "from utils import read_json, write_json\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "critique_path = 'tmp/pure_critique.json'  # 保存文件名\n",
    "batch_size = 500  # 每批的样本数\n",
    "workers_per_node = 32   # 多线程调用大模型的线程数\n",
    "model_name = \"qwen2\"  # 大模型名称\n",
    "max_tokens = 3000\n",
    "api_bases = [  # API节点\n",
    "    \"http://0.0.0.0:5001/v1\",\n",
    "    # \"http://0.0.0.0:5002/v1\",\n",
    "    # \"http://0.0.0.0:5003/v1\",\n",
    "    # \"http://0.0.0.0:5004/v1\"\n",
    "]\n",
    "print(f'data_list数量：{len(data_list)}')\n",
    "critiques = distributed_inference_parallel(\n",
    "    data_list=data_list,\n",
    "    api_bases=api_bases,\n",
    "    model_name=model_name,\n",
    "    max_tokens=max_tokens,\n",
    "    batch_size=batch_size,\n",
    "    workers_per_node=workers_per_node,\n",
    "    save_file_name=critique_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data0/anaconda3/envs/zejun7-latest/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critiques数量：566004\n",
      "不符合要求的索引数量：28\n",
      "各子判断索引数量： 5 0 26\n"
     ]
    }
   ],
   "source": [
    "\"\"\"对生成的批判进行多种规则检验，反复执行\"\"\"\n",
    "\n",
    "from utils import write_json, read_json\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/data0/zejun7/model_checkpoint/Qwen2.5-7B-Instruct')\n",
    "\n",
    "# 长度检查\n",
    "def token_len_check(critiques):\n",
    "    if critiques is None:\n",
    "        return False\n",
    "    token = len(tokenizer.encode(critiques))\n",
    "    if token < 30 or token > 1000:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 生成格式错误但被json正确解码\n",
    "def custom_check1(critiques):\n",
    "    if critiques is None:\n",
    "        return False\n",
    "    if '[rejected critiques]' in str(critiques) or '[chosen critiques]' in str(critiques):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 去除生成不完整的（没有以正确的标点符号结尾，或以...省略号结尾）\n",
    "def custom_check2(critiques):\n",
    "    if critiques is None:\n",
    "        return False\n",
    "    if not critiques:\n",
    "        return False\n",
    "    critiques = critiques.strip()\n",
    "    if critiques[-1] not in ['。', '.'] or critiques.endswith('...'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 这次没有json校验，直接对critiques进行校验\n",
    "critiques = read_json(critique_path)\n",
    "print(f'critiques数量：{len(critiques)}')\n",
    "error_idx1 = [i for i, c in enumerate(critiques) if not token_len_check(c)]  # 长度不符合要求的索引\n",
    "error_idx2 = [i for i, c in enumerate(critiques) if not custom_check1(c)]  # 生成格式错误但被json正确解码的索引\n",
    "error_idx3 = [i for i, c in enumerate(critiques) if not custom_check2(c)]  # 去除生成不完整的索引\n",
    "\n",
    "error_idx = error_idx1 + error_idx2 + error_idx3\n",
    "error_idx = sorted(list(set(error_idx)))\n",
    "print(f'不符合要求的索引数量：{len(error_idx)}')\n",
    "print(f'各子判断索引数量：', len(error_idx1), len(error_idx2), len(error_idx3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "并行处理: 跨1个节点以6批处理512项数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "批次完成进度:  67%|██████▋   | 4/6 [05:03<01:49, 54.52s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API请求失败: Request timed out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "批次完成进度:  83%|████████▎ | 5/6 [35:09<11:26, 686.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API请求失败: Request timed out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "批次完成进度: 100%|██████████| 6/6 [1:05:13<00:00, 652.24s/it] \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "**注意注意**\n",
    "step2: 如果上一步有错误的批判数量过多，可重新调用模型生成，生成完之后再执行上一步筛选错误批判\n",
    "如果数量较少可以直接丢掉，下一步可过滤 error_idx 之后保存带批判的数据集\n",
    "这块生成完之后再次运行上一个代码块，再次找出剩余的错误批判\n",
    "'''\n",
    "batch_size = 500  # 每批的样本数\n",
    "workers_per_node = 32   # 多线程调用大模型的线程数\n",
    "model_name = \"qwen2\"  # 大模型名称\n",
    "api_bases = [  # API节点\n",
    "    \"http://0.0.0.0:5001/v1\",\n",
    "    # \"http://0.0.0.0:5002/v1\",\n",
    "    # \"http://0.0.0.0:5003/v1\",\n",
    "    # \"http://0.0.0.0:5004/v1\"\n",
    "]\n",
    "data_list = [data_list[i] for i in error_idx]\n",
    "all_results = distributed_inference_parallel(\n",
    "    data_list=data_list,\n",
    "    api_bases=api_bases,\n",
    "    model_name=model_name,\n",
    "    batch_size=batch_size,\n",
    "    workers_per_node=workers_per_node,\n",
    "    save_file_name=critique_path,\n",
    ")\n",
    "\n",
    "# 赋值回原来的批判中\n",
    "for i in range(len(all_results)):\n",
    "    critiques[error_idx[i]] = all_results[i]\n",
    "write_json(critiques, critique_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sft1数据集数量：200\n",
      "新生成的自批判数量： 200\n",
      "sft2数据集数量：100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"将子批判数据整理成 成对数据格式\"\"\"\n",
    "\n",
    "sft1_dataset = read_json(file_path=train_original_path)\n",
    "print(f'sft1数据集数量：{len(sft1_dataset)}')\n",
    "\n",
    "critiques = read_json(critique_path)\n",
    "print('新生成的自批判数量：', len(critiques))\n",
    "\n",
    "sft2_dataset = []\n",
    "for i in range(0, len(sft1_dataset), 2):\n",
    "    if i in error_idx or i+1 in error_idx:\n",
    "        continue\n",
    "    sft2_dataset.append({\n",
    "        'chosen_input': sft1_dataset[i]['input'],\n",
    "        'rejected_input': sft1_dataset[i+1]['input'],\n",
    "        'chosen_critique': critiques[i],\n",
    "        'rejected_critique': critiques[i+1],\n",
    "    })\n",
    "print(f'sft2数据集数量：{len(sft2_dataset)}')\n",
    "write_json(sft2_dataset, file_path=train_selfgen_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 第二阶段权重重组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "from tqdm import tqdm\n",
    "\n",
    "'''\n",
    "完整部署分为两部分，\n",
    "1. 一个是使用vllm部署生成部分，首先调用生成批判；\n",
    "2. 另一个再使用vllm部署奖励模型，走vllm polling的方式推理得到分数，虽然vllm部署奖励模型无法加速很多，但也比transformers部署快几倍。\n",
    "\n",
    "注：目前尝试过改造vllm，使得先生成批判在经过线性层得到奖励分数，但没有成功，有厉害的大神可以尝试一下。\n",
    "'''\n",
    "\n",
    "# 输入分为原始模型和训过的模型权重\n",
    "llm_original_dir = 'Qwen/Qwen2.5-7B-Instruct'\n",
    "train_checkpoint_dir = 'save/cloud_sft2_0825/checkpoint-4500'  # 替换为你的输入目录\n",
    "# 输出目录分两个，一个是生成模型，一个是奖励模型（走vllm polling的方式推理）\n",
    "critique_checkpoint_dir = 'model_checkpoint/cloud_critique_0825'\n",
    "reward_checkpoint_dir = 'model_checkpoint/cloud_reward_0825'\n",
    "\n",
    "os.makedirs(critique_checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(reward_checkpoint_dir, exist_ok=True)\n",
    "# 将qwen2.5中的tokenizer等其他文件复制过去\n",
    "for file in os.listdir(path=llm_original_dir):\n",
    "    if file.startswith('model') or os.path.isdir(os.path.join(llm_original_dir, file)):\n",
    "        continue\n",
    "    shutil.copy(os.path.join(llm_original_dir, file), critique_checkpoint_dir)\n",
    "    shutil.copy(os.path.join(llm_original_dir, file), reward_checkpoint_dir)\n",
    "\n",
    "\n",
    "# 处理model.safetensors权重文件\n",
    "st_files = [f for f in os.listdir(train_checkpoint_dir) if f.endswith(\".safetensors\")]\n",
    "for filename in tqdm(st_files, desc=\"Processing files\"):\n",
    "    input_path = os.path.join(train_checkpoint_dir, filename)\n",
    "    critique_path = os.path.join(critique_checkpoint_dir, filename)\n",
    "    reward_path = os.path.join(reward_checkpoint_dir, filename)\n",
    "    \n",
    "    # 加载权重\n",
    "    with safe_open(input_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "        tensors = {}\n",
    "        for key in f.keys():\n",
    "            tensors[key] = f.get_tensor(key)\n",
    "    \n",
    "    # 修正键名\n",
    "    new_model_tensors = {}\n",
    "    new_reward_head_tensors = {}\n",
    "    for key, tensor in tensors.items():\n",
    "        if key.startswith(\"model.\"):\n",
    "            new_model_tensors[key[len('model.'):]] = tensor.clone().contiguous()  # 去掉开头的model.\n",
    "        elif key.startswith('reward_head.'):\n",
    "            new_reward_head_tensors[key[len('reward_head.'):]] = tensor.clone().contiguous()  # 保留其他键（如reward_head的权重）\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown key: {key}\")\n",
    "        \n",
    "    # 保存修正后的权重，奖励头部分单独放到奖励模型里\n",
    "    save_file(new_model_tensors, critique_path, metadata={\"format\": \"pt\"})\n",
    "    if new_reward_head_tensors:\n",
    "        for k, v in new_reward_head_tensors.items():\n",
    "            new_model_tensors[k] = v.clone().contiguous()\n",
    "    save_file(new_model_tensors, reward_path, metadata={\"format\": \"pt\"})\n",
    "\n",
    "\n",
    "# 处理model.safetensors.index.json索引文件\n",
    "index_path = os.path.join(train_checkpoint_dir, 'model.safetensors.index.json')\n",
    "critique_index_path = os.path.join(critique_checkpoint_dir, 'model.safetensors.index.json')\n",
    "reward_index_path = os.path.join(reward_checkpoint_dir, 'model.safetensors.index.json')\n",
    "with open(index_path, 'r') as f:\n",
    "    index_data = json.load(f)\n",
    "\n",
    "new_index_data = {}\n",
    "reward_index_data = {}\n",
    "for key, value in index_data['weight_map'].items():\n",
    "    if key.startswith('model.'):\n",
    "        new_index_data[key[len('model.'):]] = value\n",
    "    elif key.startswith('reward_head.'):\n",
    "        reward_index_data[key[len('reward_head.'):]] = value\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown key: {key}\")\n",
    "\n",
    "# 保存修正后的index.json文件\n",
    "index_data['weight_map'] = new_index_data\n",
    "with open(critique_index_path, 'w') as f:\n",
    "    json.dump(index_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# 奖励模型部分加入奖励头权重索引\n",
    "index_data.update(reward_index_data)\n",
    "with open(reward_index_path, 'w') as f:\n",
    "    json.dump(index_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# 配置奖励模型的config.json\n",
    "reward_config_path = os.path.join(reward_checkpoint_dir, 'config.json')\n",
    "with open(reward_config_path, 'r') as f:\n",
    "    reward_config = json.load(f)\n",
    "\n",
    "reward_config['architectures'] = ['Qwen2ForSequenceClassification']\n",
    "reward_config['id2label'] = {\n",
    "    \"0\": \"LABEL_0\"\n",
    "}\n",
    "reward_config['label2id'] = {\n",
    "    \"LABEL_0\": 0\n",
    "}\n",
    "\n",
    "with open(reward_config_path, 'w') as f:\n",
    "    json.dump(reward_config, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 数据测试\n",
    "\n",
    "需先部署模型和api服务，修改`cloud_infer_start.sh`中的python环境和模型路径，使用`sh cloud_infer_start.sh`启动服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "data_list = [\n",
    "    {\n",
    "        'user_prompt': 'How do I detail a car?',\n",
    "        'response': \"Detailing a car involves a thorough cleaning inside and out, as well as polishing and waxing to protect the vehicle's surfaces. Here's a step-by-step guide to detailing a car:\\n\\n**Exterior Detailing:**\\n\\n1. **Wash the Car:**\\n   - Rinse the car with water to remove loose dirt.\\n   - Use a car wash soap and microfiber wash mitt to clean the car from top to bottom.\\n   - Clean the wheels and tires with a brush and a wheel cleaner.\\n   - Rinse the car thoroughly to remove all soap.\\n\\n2. **Dry the Car:**\\n   - Use a microfiber towel or a chamois to dry the car to prevent water spots.\\n\\n3. **Clay Bar Treatment:**\\n   - Use a clay bar with a lubricant to remove embedded surface contaminants from the paint.\\n\\n4. **Polishing:**\\n   - Apply car polish with a dual-action polisher or by hand to correct paint imperfections and create a smooth surface.\\n\\n5. **Waxing:**\\n   - Apply a coat of wax or paint sealant to protect the paint and give it a glossy finish.\\n\\n6. **Windows and Mirrors:**\\n   - Clean the windows and mirrors with a glass cleaner and a microfiber towel.\\n\\n7. **Tire and Trim Dressing:**\\n   - Apply a tire dressing to the tires for a shiny finish.\\n   - Use a trim restorer or protectant on plastic and rubber parts to prevent fading.\\n\\n**Interior Detailing:**\\n\\n1. **Remove Trash:**\\n   - Clear out any trash and remove personal items from the car.\\n\\n2. **Vacuum:**\\n   - Vacuum the seats, carpets, floor mats, and trunk.\\n   - Use a brush attachment for the dashboard and door panels.\\n\\n3. **Shampoo Carpets and Upholstery:**\\n   - Use a carpet cleaner and a brush to clean the carpets and upholstery.\\n   - For leather interiors, use a leather cleaner and conditioner.\\n\\n4. **Clean Hard Surfaces:**\\n   - Wipe down all hard surfaces (dashboard, center console, door panels, etc.) with a mild all-purpose cleaner and a microfiber cloth.\\n\\n5. **Windows and Mirrors:**\\n   - Clean the interior side of windows and mirrors.\\n\\n6. **Air Vents and Crevices:**\\n   - Use a detailing brush or compressed air to clean out air vents and hard-to-reach crevices.\\n\\n7. **Final Touches:**\\n   - Apply a protectant to the dashboard and other plastic components.\\n   - Replace air fresheners if needed.\\n\\n**Additional Tips:**\\n\\n- Work in the shade or a cool, well-ventilated garage to prevent products from drying too quickly and leaving residue.\\n- Use separate buckets for washing and rinsing to avoid contaminating the clean water with dirt.\\n- Always use gentle, non-abrasive materials and cleaners specifically designed for automotive use to avoid damaging surfaces.\\n- Move in a systematic way to ensure you don't miss any spots.\\n\\nBy following these steps, you'll give your car a thorough clean that not only makes it look great but also helps in maintaining its value. Remember, regular detailing can prevent wear and tear and keep your car looking new for years to come.\"\n",
    "    }\n",
    "] * 200\n",
    "\n",
    "BASE_URL = \"http://0.0.0.0:5008\"\n",
    "\n",
    "def test_reward():\n",
    "    \"\"\"测试奖励评分\"\"\"\n",
    "    url = f\"{BASE_URL}/api/reward\"\n",
    "    data = {\n",
    "        \"user_prompt\": \"给我写一个谜语\",\n",
    "        \"response\": \"这是一个谜语：什么东西越洗越脏？答案是水。\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=data)\n",
    "    return response.json()\n",
    "\n",
    "def test_batch_reward():\n",
    "    \"\"\"测试批量奖励评分\"\"\"\n",
    "    url = f\"{BASE_URL}/api/reward/batch\"\n",
    "    data = {\n",
    "        \"data\": data_list\n",
    "    }\n",
    "    response = requests.post(url, json=data)\n",
    "    return response.json()\n",
    "\n",
    "# print(\"\\n测试奖励评分...\")\n",
    "# test_reward()\n",
    "\n",
    "# print(\"\\n测试批量奖励评分...\")\n",
    "test_batch_reward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
